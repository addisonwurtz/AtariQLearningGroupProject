{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of code used by Nick Nochnack https://github.com/nicknochnack/KerasRL-OpenAI-Atari-SpaceInvadersv0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Need very specific dependencies and environment\n",
    "My system: Intel Mac\n",
    "Commands to run in terminal in single quotes\n",
    "\n",
    "Used a virtual environment using virtualenv\n",
    "You may need to install virtualenv\n",
    "Command pulling up virtual environment with python 3.7\n",
    "'virtualenv -p python3.7 env'\n",
    "'source env/bin/activate'\n",
    "\n",
    "Update pip and pull specific versions of libraries/apis\n",
    "'pip install --upgrade pip'\n",
    "'pip install jupyter'\n",
    "'pip install stable_base_lines3==1.0'\n",
    "'pip install tensorflow==2.3.1'\n",
    "'pip install gym==0.18.0'\n",
    "'pip install keras_rl2==1.0.4'\n",
    "'pip install atari_py==0.2.9'\n",
    "'pip install autorom==0.4.2'\n",
    "'pip install open-cv-python'\n",
    "\n",
    "Download Atari 2600 ROMS:\n",
    "type this command in terminal: 'autorom'\n",
    "fill in current_path and current_directory\n",
    "change directories command: 'cd /{current_path}/{current_directory}/env/lib/python3.7/site_packages/atari_py/atari_roms/'\n",
    "once in correct directory type in command: 'python -m atari_py.import_roms .'\n",
    "cd back into current_directory\n",
    "\n",
    "run jupyter notebook with command: 'jupyter notebook'\n",
    "Fill and run cells below\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd059ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.callbacks import ModelIntervalCheckpoint\n",
    "\n",
    "WEIGHTS_FILENAME = f'spaceinv_weights.h5f'\n",
    "CHECKPOINT_WEIGHTS_FILENAME = 'spaceinv_weights_{step}.h5f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a0f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds deep learning model, 3 convolution and 3 full layers\n",
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c5d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds DQN model with annealing policy and sequential memory\n",
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf31cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 15:25:21.360972: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-08 15:25:21.386083: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff0909d37f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-08 15:25:21.386098: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "WARNING:tensorflow:From /Users/jiggins33/dev/ai/ai_final/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "  655/10000: episode: 1, duration: 10.996s, episode steps: 655, steps per second:  60, episode reward: 155.000, mean reward:  0.237 [ 0.000, 30.000], mean action: 2.449 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 1351/10000: episode: 2, duration: 236.486s, episode steps: 696, steps per second:   3, episode reward: 155.000, mean reward:  0.223 [ 0.000, 30.000], mean action: 2.503 [0.000, 5.000],  loss: 79.571968, mean_q: 10.693740, mean_eps: 0.894205\n",
      " 2220/10000: episode: 3, duration: 565.602s, episode steps: 869, steps per second:   2, episode reward: 210.000, mean reward:  0.242 [ 0.000, 30.000], mean action: 2.540 [0.000, 5.000],  loss: 1.273533, mean_q: 8.856466, mean_eps: 0.839350\n",
      " 3215/10000: episode: 4, duration: 640.592s, episode steps: 995, steps per second:   2, episode reward: 260.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 2.446 [0.000, 5.000],  loss: 0.863428, mean_q: 9.254765, mean_eps: 0.755470\n",
      " 3707/10000: episode: 5, duration: 312.831s, episode steps: 492, steps per second:   2, episode reward: 80.000, mean reward:  0.163 [ 0.000, 25.000], mean action: 2.445 [0.000, 5.000],  loss: 0.853335, mean_q: 8.980855, mean_eps: 0.688555\n",
      " 4267/10000: episode: 6, duration: 367.014s, episode steps: 560, steps per second:   2, episode reward: 60.000, mean reward:  0.107 [ 0.000, 15.000], mean action: 2.536 [0.000, 5.000],  loss: 0.737592, mean_q: 8.661148, mean_eps: 0.641215\n",
      " 4843/10000: episode: 7, duration: 384.315s, episode steps: 576, steps per second:   1, episode reward: 90.000, mean reward:  0.156 [ 0.000, 30.000], mean action: 2.543 [0.000, 5.000],  loss: 0.413574, mean_q: 9.014659, mean_eps: 0.590095\n",
      " 5334/10000: episode: 8, duration: 323.477s, episode steps: 491, steps per second:   2, episode reward: 65.000, mean reward:  0.132 [ 0.000, 20.000], mean action: 2.432 [0.000, 5.000],  loss: 0.527472, mean_q: 9.268837, mean_eps: 0.542080\n",
      " 5844/10000: episode: 9, duration: 333.387s, episode steps: 510, steps per second:   2, episode reward: 45.000, mean reward:  0.088 [ 0.000, 30.000], mean action: 2.243 [0.000, 5.000],  loss: 0.299332, mean_q: 9.081473, mean_eps: 0.497035\n"
     ]
    }
   ],
   "source": [
    "# Run up environment, model, agent, and train agent\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "num_steps = 10000\n",
    "\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "model = build_model(height, width, channels, actions)\n",
    "model.summary()\n",
    "\n",
    "callbacks = [ModelIntervalCheckpoint(CHECKPOINT_WEIGHTS_FILENAME, interval=5000)]\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=0.00025))\n",
    "\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=num_steps, visualize=False, verbose=2)\n",
    "print('done')\n",
    "dqn.save_weights(WEIGHTS_FILENAME, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "883ef329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just trained for 10000 number of steps\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 80.000, steps: 672\n",
      "Episode 2: reward: 245.000, steps: 1090\n",
      "Episode 3: reward: 135.000, steps: 704\n",
      "Episode 4: reward: 145.000, steps: 887\n",
      "Episode 5: reward: 75.000, steps: 513\n",
      "Episode 6: reward: 110.000, steps: 875\n",
      "Episode 7: reward: 105.000, steps: 912\n",
      "Episode 8: reward: 165.000, steps: 1070\n",
      "Episode 9: reward: 75.000, steps: 390\n",
      "Episode 10: reward: 55.000, steps: 397\n",
      "119.0\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(f'Just trained for {num_steps} number of steps')\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55893e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
